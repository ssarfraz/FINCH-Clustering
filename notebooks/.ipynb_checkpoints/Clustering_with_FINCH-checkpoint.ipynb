{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3603470a-fed8-4d9e-bd5b-74091166f933",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demo: Clustering STL-10 data with FINCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6f7bc-e4f8-4d16-9c77-c32b904ca06b",
   "metadata": {},
   "source": [
    "The notebook demonstrate a common usage example of clustering a dataset with FINCH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96187e48-0d77-4912-82e3-9626d388168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from finch import FINCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f09941-735a-47f7-b0ae-2d2c2e6fb3bb",
   "metadata": {},
   "source": [
    "# Load STL-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871ddc3-1e3a-48cf-8dbd-dfa8551c69c3",
   "metadata": {},
   "source": [
    "We load the STL-10 data which contains resent50 features of 13000 samples. The data is included in the github repo in the data folder. Since this data is in a mat file format, we will use h5py to load it as an numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a38e67bb-a1c2-4b5c-87ac-004f4f4a6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../data/STL-10/data.mat', 'r')\n",
    "data = f.get('data')\n",
    "data = np.array(data).T\n",
    "f = h5py.File('../data/STL-10/labels.mat', 'r')\n",
    "gt = f.get('labels')\n",
    "gt = np.squeeze(np.array(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068d0f02-becc-4f3a-99f7-e1fbb9f9ef7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 2048)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff115f-af2e-4076-ae52-6b1d0f05636c",
   "metadata": {},
   "source": [
    "# Cluster with FINCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd56f37-7c7f-410b-91d9-8ff46377350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: 2061 clusters\n",
      "Partition 1: 177 clusters\n",
      "Partition 2: 37 clusters\n",
      "Partition 3: 10 clusters\n",
      "Partition 4: 2 clusters\n"
     ]
    }
   ],
   "source": [
    "c, num_clust, req_c = FINCH(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91ce3da9-2111-4597-b1fd-9fb2f6364429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 5)\n",
      "[2061, 177, 37, 10, 2]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(c.shape)\n",
    "print(num_clust)\n",
    "print(req_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c529be6-bc3e-42d3-a69b-4b2622d5b3d0",
   "metadata": {},
   "source": [
    "It returns the cluster labels for each partition in the variable 'c' which is of size (N x numPartitions) e.g., (13000, 5) in this case. Each column in array c provides cluster labels for that partition.\n",
    "\n",
    "num_clust provides how many cluster it has produced in each partition or step of the run. As you see above: num_clust = 2061, 177, 37, 10, 2 indicating it found 2061 clusters in partition 0, 177 in partition 2 and so on to 10 clusters in step 4. We can pick the respective cluster labels for the data in the returned array 'c'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f2871-53bf-44d2-b018-8c7ae9039f2b",
   "metadata": {},
   "source": [
    "## Evaluate quality of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4190873-9c5e-42cb-a2ae-a0941a5e37da",
   "metadata": {},
   "source": [
    "For example, here since the fourth partition c[:, 3] provides labels for a 10 clustering result. we will evaluate its quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "787bff8d-f698-4914-a967-ae15859c0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI Score: 85.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "score = nmi(gt, c[:, 3])\n",
    "print('NMI Score: {:.2f}'.format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57189620-a8e3-424c-a2d4-5856aec914a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Required number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabd36d-1ae9-4806-8d88-1fa685359190",
   "metadata": {},
   "source": [
    "In many application we need to cluster a dataset to obtain a specifioed number of clusters. This can be done easily with FINCH by setting the input param req_clust. For example lets cluster the dataset to also a get a 15 cluster partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43312ebd-0fc6-4aec-ac8c-bc214f06aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c, num_clust, req_c = FINCH(data, req_clust=15, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2891a-81a9-42b0-a206-624782594ada",
   "metadata": {},
   "source": [
    "Here the variable req_c will contain the labels of the 15 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ceca6ea-d2ba-45a8-b1cc-e6c8498f7802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000,)\n",
      "[0 1 1 ... 6 6 6]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "print(req_c.shape)\n",
    "print(req_c)\n",
    "print(np.unique(req_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839eaaa8-f6b7-4c93-900a-56cad7e3430d",
   "metadata": {},
   "source": [
    "### Run FINCH on large data using approximate nearest neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550c4a8-ed8a-4ea0-82c2-470604c47fe7",
   "metadata": {},
   "source": [
    "To run on large scale data, computing the the nearest enghbour is not possible using exact distances, here an approximate nerest neighbour method such as pynndescent can be used. This is also useful for low compute machines with small memory.\n",
    "\n",
    "We can specify number of samples above which it uses pynndesent to compute nearest neighbor distances. Since our data has 13000 samples we force it to use pynndescent by specifying use_ann_above_samples parameter to 10000 i.e any data with above 10000 samples will be processed by computing distances with pynndescent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bafae936-6691-45ae-8b86-38646539d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyNNDescent to compute 1st-neighbours at this step ...\n",
      "Step PyNNDescent done ...\n",
      "Partition 0: 2079 clusters\n",
      "Partition 1: 192 clusters\n",
      "Partition 2: 35 clusters\n",
      "Partition 3: 11 clusters\n",
      "Partition 4: 2 clusters\n"
     ]
    }
   ],
   "source": [
    "c, num_clust, req_c = FINCH(data, req_clust=None, use_ann_above_samples=10000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96dc27c-519d-4bfe-94ba-ff71b527c663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
